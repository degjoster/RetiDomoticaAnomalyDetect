{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reti Domotica Anomaly Detection - Team 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componenti del Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Francesco CATANIA\n",
    "- Giorgio DEIANA\n",
    "- Marco SASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisiti di sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Installare le librerie indicate nel file \"requirements.txt\"\n",
    "- Installare ODBC Driver for SQL Server (https://docs.microsoft.com/it-it/sql/connect/odbc/download-odbc-driver-for-sql-server?view=sql-server-ver15)\n",
    "- All'interno del file \"settings.json\" inserire le seguenti stringhe: \"python.pythonPath\": \"{NOMEAMBIENTE}\\\\Scripts\\\\python.exe\",\n",
    "    \"python.linting.pylintPath\": \"{NOMEAMBIENTE}\\\\bin\\\\pylint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note Metodologiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modello adottato e addestramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algoritmo di Machine Learning utilizzato per la rilevazione delle anomalie è l'Isolation Forest implementato all'interno della libreria Scikit-Learn. Si è scelto di addestrare cinque differenti modelli di Machine Learning - uno per ciascuna unità di misura rilevata ('ppm', '%', 'C°', 'W', 'Wh') - al fine di separare opportunamente i processi di ricerca di eventuali anomalie a seconda delle tipologie di rilevazioni presenti nel dataset \"log\". Dal progetto sono state esclusi i dati di tipo booleano riguardanti la rilevazione delle presenze.\n",
    "\n",
    "I modelli sopracitati sono stati addestrati eseguendo un apposito notebook Python all'interno della piattaforma Databricks di Microsoft Azure, consultabile nella directory *Notebooks* della repository GitHub. I dati per l'addestramento sono stati estratti dal database SQL *CampusData* presente tra le risorse della sottoscrizione Microsoft Azure del corso.\n",
    "\n",
    "Per il training di ciascun modello sono state considerate features in grado di cogliere il valore, la tipologia e l'eventuale stagionalità delle rilevazioni effettuate dai sensori del Campus, nello specifico:\n",
    "\n",
    "- mese della rilevazione (derivata dal campo *Data* della tabella *dbo.Log* del database SQL)\n",
    "- giorno della settimana della rilevazione (derivata dal campo *Data* del dataset *dbo.Log* del database SQL)\n",
    "- valore della rilevazione (feature *Value* della tabella \"dbo.Log\" del database SQL)\n",
    "- tipologia di rilevazione (feature *Id* della tabella *dbo.Group* del database SQL, ottenuta tramite join con la tabella *dbo.Log*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Piattaforma Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utente può interagire con le funzionalità implementate nel progetto attraverso un'applicazione web realizzata in Streamlit che permette di:\n",
    "\n",
    "- caricare nuovi dati dal database SQL Azure - diversi da quelli utilizzati per l'addestramento dei modelli - per effettuare delle nuove predizioni volte ad individuare eventuali anomalie;\n",
    "- simulare una rilevazione di un sensore indicando la data, la tipologia di dato e il valore numerico per determinare, grazie alle predizioni dei modelli di Machine Learning, se si tratta di un'anomalia o meno.\n",
    "\n",
    "Per l'accesso all'applicazione:\n",
    "1. da terminale, digitare ***cd RetiDomoticaAnomalyDetect\\DomoticaStreamLit*** per posizionarsi all'interno della directory *DomoticaStreamLit*\n",
    "2. sempre da terminale, digitare ***streamlit run app.py*** per lanciare l'applicazione\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvataggio delle predizioni su Azure Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le predizioni ottenute via Streamlit secondo le due modalità illustrate nel paragrafo precedente vengono archiviate in formato JSON all'interno di un apposito Azure Storage denominato *sorageaccountgdeiana*. \n",
    "Nello specifico, le predizioni frutto della lettura da database SQL Azure vengono salvate nel container *predictcontainerdomoticaadsqldb*, mentre quelle generate dalla simulazione dell'utente vengono archiviate all'interno del container *predictcontainerdomoticaaduser*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigazione cartelle della repository GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le varie componenti del progetto sono organizzate secondo la seguente struttura di cartelle:\n",
    "\n",
    "- **Datasets** contiene i file *building.csv*, *group.csv* e *log2.csv* messi a disposizione a inizio progetto per le analisi preliminari in locale;\n",
    "\n",
    "\n",
    "- **DomoticaStreamLit** contiene il file Python *app.py* per il lancio dell'applicazione web in Streamlit; \n",
    "\n",
    "\n",
    "- **Notebooks** contiene i notebook utilizzati per l'esplorazione preliminare del dataset e per il training dei modelli in Databricks; \n",
    "\n",
    "\n",
    "- **TeamLibraries** contiene le funzioni Python sviluppate internamente dal team, suddivise in tre diversi moduli:\n",
    "    - *LoadData.py* racchiude le funzioni per la connessione al database SQL Azure, lo scarico dei dati e il caricamento dei modelli versionati in Databricks;\n",
    "    \n",
    "    - *ModelFunctions.py* comprende le funzioni per il preprocessing dei dati e la predizione tramite i modelli;\n",
    "    \n",
    "    - *OperationsStorageAccount.py* contiene le funzioni per la connessione allo Storage Account Azure e il salvataggio delle predizioni in formato JSON negli appositi container;\n",
    "    \n",
    "    ***A causa di imprevisti tecnici che si sono verificati con il serving dei modelli in Databricks, la cartella TeamLibraries contiene un backup di tutti i modelli in formato Pickle addestrati in locale***.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
