{"cells":[{"cell_type":"code","source":["pip install mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e466ade-bb87-484e-936c-18b4cb1647b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-1.12.1-py3-none-any.whl (13.9 MB)\nCollecting docker&gt;=4.0.0\n  Downloading docker-4.4.0-py2.py3-none-any.whl (146 kB)\nCollecting protobuf&gt;=3.6.0\n  Downloading protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\nCollecting alembic&lt;=1.4.1\n  Downloading alembic-1.4.1.tar.gz (1.1 MB)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.7/site-packages (from mlflow) (2.8.1)\nCollecting click&gt;=7.0\n  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\nCollecting querystring-parser\n  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.7/site-packages (from mlflow) (1.14.0)\nCollecting databricks-cli&gt;=0.8.7\n  Downloading databricks-cli-0.14.1.tar.gz (54 kB)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.7/site-packages (from mlflow) (2.25.0)\nCollecting gunicorn; platform_system != &#34;Windows&#34;\n  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\nCollecting azure-storage-blob\n  Downloading azure_storage_blob-12.6.0-py2.py3-none-any.whl (328 kB)\nCollecting sqlalchemy\n  Downloading SQLAlchemy-1.3.20-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\nCollecting Flask\n  Using cached Flask-1.1.2-py2.py3-none-any.whl (94 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.7/site-packages (from mlflow) (1.1.4)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.7/site-packages (from mlflow) (5.3.1)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.7/site-packages (from mlflow) (1.18.1)\nCollecting gitpython&gt;=2.1.0\n  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.18.1.tar.gz (21 kB)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.7/site-packages (from mlflow) (0.3)\nCollecting sqlparse&gt;=0.3.1\n  Using cached sqlparse-0.4.1-py3-none-any.whl (42 kB)\nCollecting cloudpickle\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\nCollecting websocket-client&gt;=0.32.0\n  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\nCollecting Mako\n  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\nCollecting python-editor&gt;=0.3\n  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\nCollecting tabulate&gt;=0.7.7\n  Using cached tabulate-0.8.7-py3-none-any.whl (24 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2020.6.20)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2.8)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (1.25.8)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/lib/python3/dist-packages (from requests&gt;=2.17.3-&gt;mlflow) (3.0.4)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn; platform_system != &#34;Windows&#34;-&gt;mlflow) (45.2.0)\nCollecting msrest&gt;=0.6.10\n  Downloading msrest-0.6.19-py2.py3-none-any.whl (84 kB)\nCollecting azure-core&lt;2.0.0,&gt;=1.9.0\n  Using cached azure_core-1.9.0-py2.py3-none-any.whl (124 kB)\nRequirement already satisfied: cryptography&gt;=2.1.4 in /databricks/python3/lib/python3.7/site-packages (from azure-storage-blob-&gt;mlflow) (2.8)\nCollecting itsdangerous&gt;=0.24\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: Jinja2&gt;=2.10.1 in /databricks/python3/lib/python3.7/site-packages (from Flask-&gt;mlflow) (2.11.2)\nCollecting Werkzeug&gt;=0.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\nRequirement already satisfied: pytz&gt;=2017.2 in /databricks/python3/lib/python3.7/site-packages (from pandas-&gt;mlflow) (2019.3)\nCollecting gitdb&lt;5,&gt;=4.0.1\n  Using cached gitdb-4.0.5-py3-none-any.whl (63 kB)\nRequirement already satisfied: prometheus_client in /databricks/python3/lib/python3.7/site-packages (from prometheus-flask-exporter-&gt;mlflow) (0.9.0)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in /databricks/python3/lib/python3.7/site-packages (from Mako-&gt;alembic&lt;=1.4.1-&gt;mlflow) (1.1.1)\nCollecting isodate&gt;=0.6.0\n  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\nCollecting requests-oauthlib&gt;=0.5.0\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: cffi!=1.11.3,&gt;=1.8 in /databricks/python3/lib/python3.7/site-packages (from cryptography&gt;=2.1.4-&gt;azure-storage-blob-&gt;mlflow) (1.14.0)\nCollecting smmap&lt;4,&gt;=3.0.1\n  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\nCollecting oauthlib&gt;=3.0.0\n  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.7/site-packages (from cffi!=1.11.3,&gt;=1.8-&gt;cryptography&gt;=2.1.4-&gt;azure-storage-blob-&gt;mlflow) (2.19)\nBuilding wheels for collected packages: alembic, databricks-cli, prometheus-flask-exporter\n  Building wheel for alembic (setup.py): started\n  Building wheel for alembic (setup.py): finished with status &#39;done&#39;\n  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158154 sha256=0fa688c38637690df40032d8b052cd86d5e2426629c824a9e369856363631d1b\n  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status &#39;done&#39;\n  Created wheel for databricks-cli: filename=databricks_cli-0.14.1-py3-none-any.whl size=100576 sha256=cad5243ae3651fb6b22a269e0a7cf52423d8c8f8fd2bac3ffb6aa6a1c73ba9e4\n  Stored in directory: /root/.cache/pip/wheels/7f/d9/25/baefac3eda0e7dbf143008d2b9865e0d923d4b7306136244fe\n  Building wheel for prometheus-flask-exporter (setup.py): started\n  Building wheel for prometheus-flask-exporter (setup.py): finished with status &#39;done&#39;\n  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-py3-none-any.whl size=17157 sha256=588734bfac9775fec7f670edb44b52ee62de888afac56748f6a4d82e4d2afa36\n  Stored in directory: /root/.cache/pip/wheels/c4/b6/b5/e76659f3b2a3a226565e27f0a7eb7a3ac93c3f4d68acfbe617\nSuccessfully built alembic databricks-cli prometheus-flask-exporter\nInstalling collected packages: websocket-client, docker, protobuf, sqlalchemy, Mako, python-editor, alembic, click, querystring-parser, tabulate, databricks-cli, gunicorn, isodate, oauthlib, requests-oauthlib, msrest, azure-core, azure-storage-blob, itsdangerous, Werkzeug, Flask, smmap, gitdb, gitpython, prometheus-flask-exporter, sqlparse, cloudpickle, mlflow\nSuccessfully installed Flask-1.1.2 Mako-1.1.3 Werkzeug-1.0.1 alembic-1.4.1 azure-core-1.9.0 azure-storage-blob-12.6.0 click-7.1.2 cloudpickle-1.6.0 databricks-cli-0.14.1 docker-4.4.0 gitdb-4.0.5 gitpython-3.1.11 gunicorn-20.0.4 isodate-0.6.0 itsdangerous-1.1.0 mlflow-1.12.1 msrest-0.6.19 oauthlib-3.1.0 prometheus-flask-exporter-0.18.1 protobuf-3.14.0 python-editor-1.0.4 querystring-parser-1.2.4 requests-oauthlib-1.3.0 smmap-3.0.4 sqlalchemy-1.3.20 sqlparse-0.4.1 tabulate-0.8.7 websocket-client-0.57.0\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-1.12.1-py3-none-any.whl (13.9 MB)\nCollecting docker&gt;=4.0.0\n  Downloading docker-4.4.0-py2.py3-none-any.whl (146 kB)\nCollecting protobuf&gt;=3.6.0\n  Downloading protobuf-3.14.0-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\nCollecting alembic&lt;=1.4.1\n  Downloading alembic-1.4.1.tar.gz (1.1 MB)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.7/site-packages (from mlflow) (2.8.1)\nCollecting click&gt;=7.0\n  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\nCollecting querystring-parser\n  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.7/site-packages (from mlflow) (1.14.0)\nCollecting databricks-cli&gt;=0.8.7\n  Downloading databricks-cli-0.14.1.tar.gz (54 kB)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.7/site-packages (from mlflow) (2.25.0)\nCollecting gunicorn; platform_system != &#34;Windows&#34;\n  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\nCollecting azure-storage-blob\n  Downloading azure_storage_blob-12.6.0-py2.py3-none-any.whl (328 kB)\nCollecting sqlalchemy\n  Downloading SQLAlchemy-1.3.20-cp37-cp37m-manylinux2010_x86_64.whl (1.3 MB)\nCollecting Flask\n  Using cached Flask-1.1.2-py2.py3-none-any.whl (94 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.7/site-packages (from mlflow) (1.1.4)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.7/site-packages (from mlflow) (5.3.1)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.7/site-packages (from mlflow) (1.18.1)\nCollecting gitpython&gt;=2.1.0\n  Downloading GitPython-3.1.11-py3-none-any.whl (159 kB)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.18.1.tar.gz (21 kB)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.7/site-packages (from mlflow) (0.3)\nCollecting sqlparse&gt;=0.3.1\n  Using cached sqlparse-0.4.1-py3-none-any.whl (42 kB)\nCollecting cloudpickle\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\nCollecting websocket-client&gt;=0.32.0\n  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\nCollecting Mako\n  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\nCollecting python-editor&gt;=0.3\n  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\nCollecting tabulate&gt;=0.7.7\n  Using cached tabulate-0.8.7-py3-none-any.whl (24 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2020.6.20)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2.8)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (1.25.8)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/lib/python3/dist-packages (from requests&gt;=2.17.3-&gt;mlflow) (3.0.4)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.7/dist-packages (from gunicorn; platform_system != &#34;Windows&#34;-&gt;mlflow) (45.2.0)\nCollecting msrest&gt;=0.6.10\n  Downloading msrest-0.6.19-py2.py3-none-any.whl (84 kB)\nCollecting azure-core&lt;2.0.0,&gt;=1.9.0\n  Using cached azure_core-1.9.0-py2.py3-none-any.whl (124 kB)\nRequirement already satisfied: cryptography&gt;=2.1.4 in /databricks/python3/lib/python3.7/site-packages (from azure-storage-blob-&gt;mlflow) (2.8)\nCollecting itsdangerous&gt;=0.24\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: Jinja2&gt;=2.10.1 in /databricks/python3/lib/python3.7/site-packages (from Flask-&gt;mlflow) (2.11.2)\nCollecting Werkzeug&gt;=0.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\nRequirement already satisfied: pytz&gt;=2017.2 in /databricks/python3/lib/python3.7/site-packages (from pandas-&gt;mlflow) (2019.3)\nCollecting gitdb&lt;5,&gt;=4.0.1\n  Using cached gitdb-4.0.5-py3-none-any.whl (63 kB)\nRequirement already satisfied: prometheus_client in /databricks/python3/lib/python3.7/site-packages (from prometheus-flask-exporter-&gt;mlflow) (0.9.0)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in /databricks/python3/lib/python3.7/site-packages (from Mako-&gt;alembic&lt;=1.4.1-&gt;mlflow) (1.1.1)\nCollecting isodate&gt;=0.6.0\n  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\nCollecting requests-oauthlib&gt;=0.5.0\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nRequirement already satisfied: cffi!=1.11.3,&gt;=1.8 in /databricks/python3/lib/python3.7/site-packages (from cryptography&gt;=2.1.4-&gt;azure-storage-blob-&gt;mlflow) (1.14.0)\nCollecting smmap&lt;4,&gt;=3.0.1\n  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\nCollecting oauthlib&gt;=3.0.0\n  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.7/site-packages (from cffi!=1.11.3,&gt;=1.8-&gt;cryptography&gt;=2.1.4-&gt;azure-storage-blob-&gt;mlflow) (2.19)\nBuilding wheels for collected packages: alembic, databricks-cli, prometheus-flask-exporter\n  Building wheel for alembic (setup.py): started\n  Building wheel for alembic (setup.py): finished with status &#39;done&#39;\n  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158154 sha256=0fa688c38637690df40032d8b052cd86d5e2426629c824a9e369856363631d1b\n  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n  Building wheel for databricks-cli (setup.py): started\n  Building wheel for databricks-cli (setup.py): finished with status &#39;done&#39;\n  Created wheel for databricks-cli: filename=databricks_cli-0.14.1-py3-none-any.whl size=100576 sha256=cad5243ae3651fb6b22a269e0a7cf52423d8c8f8fd2bac3ffb6aa6a1c73ba9e4\n  Stored in directory: /root/.cache/pip/wheels/7f/d9/25/baefac3eda0e7dbf143008d2b9865e0d923d4b7306136244fe\n  Building wheel for prometheus-flask-exporter (setup.py): started\n  Building wheel for prometheus-flask-exporter (setup.py): finished with status &#39;done&#39;\n  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-py3-none-any.whl size=17157 sha256=588734bfac9775fec7f670edb44b52ee62de888afac56748f6a4d82e4d2afa36\n  Stored in directory: /root/.cache/pip/wheels/c4/b6/b5/e76659f3b2a3a226565e27f0a7eb7a3ac93c3f4d68acfbe617\nSuccessfully built alembic databricks-cli prometheus-flask-exporter\nInstalling collected packages: websocket-client, docker, protobuf, sqlalchemy, Mako, python-editor, alembic, click, querystring-parser, tabulate, databricks-cli, gunicorn, isodate, oauthlib, requests-oauthlib, msrest, azure-core, azure-storage-blob, itsdangerous, Werkzeug, Flask, smmap, gitdb, gitpython, prometheus-flask-exporter, sqlparse, cloudpickle, mlflow\nSuccessfully installed Flask-1.1.2 Mako-1.1.3 Werkzeug-1.0.1 alembic-1.4.1 azure-core-1.9.0 azure-storage-blob-12.6.0 click-7.1.2 cloudpickle-1.6.0 databricks-cli-0.14.1 docker-4.4.0 gitdb-4.0.5 gitpython-3.1.11 gunicorn-20.0.4 isodate-0.6.0 itsdangerous-1.1.0 mlflow-1.12.1 msrest-0.6.19 oauthlib-3.1.0 prometheus-flask-exporter-0.18.1 protobuf-3.14.0 python-editor-1.0.4 querystring-parser-1.2.4 requests-oauthlib-1.3.0 smmap-3.0.4 sqlalchemy-1.3.20 sqlparse-0.4.1 tabulate-0.8.7 websocket-client-0.57.0\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import mlflow\nimport mlflow.sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nimport pickle\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import IsolationForest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77a6da77-95a7-4aa1-91c4-de40379aead8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Caricamento dei dati in PySpark e conversione in dataframe Pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42d132d9-59fc-4d3d-bfdd-2fcaaf143686"}}},{"cell_type":"code","source":["# File location and type\nfile_location_group = \"dbfs:/FileStore/shared_uploads/marco.sasso@reti.it/group.csv\"\nfile_location_building = \"dbfs:/FileStore/shared_uploads/marco.sasso@reti.it/building.csv\"\nfile_location_log = \"dbfs:/FileStore/shared_uploads/marco.sasso@reti.it/log2.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\nencoder = \"cp1252\"\nvirgola = \",\"\npunto_e_virgola = \";\"\n\n# Loading files\ndf_group = spark.read.format(file_type).option(\"inferSchema\", infer_schema).option(\"header\", first_row_is_header).option(\"sep\", virgola).option(\"encoding\", encoder).load(file_location_group)\ndf_group = df_group.toPandas()\n\ndf_building = spark.read.format(file_type).option(\"inferSchema\", infer_schema).option(\"header\", first_row_is_header).option(\"sep\", virgola).option(\"encoding\", encoder).load(file_location_building)\ndf_building = df_building.toPandas()\n\ndf_log = spark.read.format(file_type).option(\"inferSchema\", infer_schema).option(\"header\", first_row_is_header).option(\"sep\", punto_e_virgola).option(\"encoding\", encoder).load(file_location_log)\ndf_log = df_log.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"321da03d-bc4c-4497-b871-f9214ef4273b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Definizione delle funzioni principali per il preprocessing e il training del modello"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3409e533-388e-4da3-8cc5-b984e4801ce5"}}},{"cell_type":"code","source":["def signals_selection(logs, groups):\n    \n    # Rimuove le rilevazioni di tipo boolean, quelle non mappate nel dataset \"group\" e quelle che non presentano un valore nei\n    # campi \"Data\", \"Value\" e \"Id\". Restituisce un dataframe di segnali che saranno effettivamente preprocessati e passati\n    # come input al modello per l'addestramento\n    \n    # Unità di misura da tenere e tipologie di rilevazioni da rimuovere\n    measures_to_keep = ['ppm', 'C°', '%', 'W', 'Wh']\n    description_to_remove = ['Daikin Active Power Total','Consumo enel di E1 + Villa']\n    \n     # Rimuovo i record associati a rilevazioni di tipo booleano\n    logs = logs[logs['ValueType'].isin(measures_to_keep) == True]\n    \n    # Rimuovo i record associati a rilevazioni di tipo \"Daikin Active Power Total\" e \"Consumo Enel di E1 + Villa\"\n    logs = logs[logs['Description'].isin(description_to_remove) == False]\n    \n    # Effettuo il left join con la tabella group per ricavare l'ID della rilevazione\n    logs = logs.merge(groups[['Description', 'Id']], how='left', on='Description')\n    logs = logs.dropna(subset=['Data', 'Id', 'Value']).reset_index(drop=True)\n    \n    # Converto la colonna 'Data' in formato datetime\n    logs['Data'] = pd.to_datetime(logs['Data'])\n    \n    # Converto la colonna 'Value' in formato numerico\n    logs[\"Value\"] = pd.to_numeric(np.char.replace(logs['Value'].to_numpy().astype(str),',','.'))\n    \n    return logs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d0ebdbc-ebb0-45d4-993e-34738cf762be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def preprocessing(sel_logs):\n    \n    # Ricavo il mese e il giorno della settimana della rilevazione\n    sel_logs['Month'] = pd.DatetimeIndex(sel_logs['Data']).month\n    sel_logs['Weekday'] = pd.DatetimeIndex(sel_logs['Data']).weekday\n    \n    # 3) Rimuovo le features che non servono ad addestrare il modello\n    features_to_drop = ['Data', 'IdBuilding', 'IndividualAddress', 'GroupAddress', 'TelegramType', 'ValueType', 'Description']\n    sel_logs = sel_logs.drop(columns=features_to_drop)   \n    \n    return sel_logs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1f2f517-799e-48e3-8602-1f739cdf0f86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def forest_train_save(train_logs, num_est=100, cnt_rate=0.01):\n    \n    \n    # INSERIRE AUTOLOG MLFLOW\n    \n    # Abilito l'autolog di ML Flow \n    mlflow.sklearn.autolog()\n    \n    with mlflow.start_run():\n      iso_forest = IsolationForest(n_estimators=num_est, random_state=19, contamination=cnt_rate, behaviour='deprecated').fit(train_logs)\n      \n    return True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2febd04b-310f-450b-8340-eb762196679a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Run del training del modello"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe33ff63-bcc6-4d7d-b802-e5ef06931d08"}}},{"cell_type":"code","source":["sel_logs = signals_selection(df_log, df_group)\nsel_logs.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75a1b509-57c9-4152-9803-d1471c67d1ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Data</th>\n      <th>IdBuilding</th>\n      <th>IndividualAddress</th>\n      <th>GroupAddress</th>\n      <th>TelegramType</th>\n      <th>Value</th>\n      <th>ValueType</th>\n      <th>Description</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-02-22 12:27:53.323333300</td>\n      <td>1.0</td>\n      <td>3.1.6</td>\n      <td>2/2/0</td>\n      <td>write</td>\n      <td>-33429.70000</td>\n      <td>W</td>\n      <td>Produzione totale fotovoltaico</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-02-22 12:28:56.400000000</td>\n      <td>1.0</td>\n      <td>3.1.201</td>\n      <td>2/0/3</td>\n      <td>write</td>\n      <td>35.68625</td>\n      <td>%</td>\n      <td>C1/0/M102 Umidità</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-02-22 12:29:37.176666700</td>\n      <td>1.0</td>\n      <td>3.2.202</td>\n      <td>2/0/0</td>\n      <td>write</td>\n      <td>400.96000</td>\n      <td>ppm</td>\n      <td>C1/0/M101 CO2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-02-22 12:30:21.933333300</td>\n      <td>1.0</td>\n      <td>3.3.201</td>\n      <td>2/1/8</td>\n      <td>write</td>\n      <td>36.86272</td>\n      <td>%</td>\n      <td>C1/0/U105 Umidità</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-02-22 12:31:23.480000000</td>\n      <td>1.0</td>\n      <td>3.3.202</td>\n      <td>2/1/0</td>\n      <td>write</td>\n      <td>536.96000</td>\n      <td>ppm</td>\n      <td>C1/0/U101 C02</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[10]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Data</th>\n      <th>IdBuilding</th>\n      <th>IndividualAddress</th>\n      <th>GroupAddress</th>\n      <th>TelegramType</th>\n      <th>Value</th>\n      <th>ValueType</th>\n      <th>Description</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-02-22 12:27:53.323333300</td>\n      <td>1.0</td>\n      <td>3.1.6</td>\n      <td>2/2/0</td>\n      <td>write</td>\n      <td>-33429.70000</td>\n      <td>W</td>\n      <td>Produzione totale fotovoltaico</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-02-22 12:28:56.400000000</td>\n      <td>1.0</td>\n      <td>3.1.201</td>\n      <td>2/0/3</td>\n      <td>write</td>\n      <td>35.68625</td>\n      <td>%</td>\n      <td>C1/0/M102 Umidità</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-02-22 12:29:37.176666700</td>\n      <td>1.0</td>\n      <td>3.2.202</td>\n      <td>2/0/0</td>\n      <td>write</td>\n      <td>400.96000</td>\n      <td>ppm</td>\n      <td>C1/0/M101 CO2</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-02-22 12:30:21.933333300</td>\n      <td>1.0</td>\n      <td>3.3.201</td>\n      <td>2/1/8</td>\n      <td>write</td>\n      <td>36.86272</td>\n      <td>%</td>\n      <td>C1/0/U105 Umidità</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-02-22 12:31:23.480000000</td>\n      <td>1.0</td>\n      <td>3.3.202</td>\n      <td>2/1/0</td>\n      <td>write</td>\n      <td>536.96000</td>\n      <td>ppm</td>\n      <td>C1/0/U101 C02</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["train_logs = preprocessing(sel_logs)\ntrain_logs.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"918b03f8-9f8e-4dcd-9771-dea0883901d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n      <th>Id</th>\n      <th>Month</th>\n      <th>Weekday</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-33429.70000</td>\n      <td>13.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35.68625</td>\n      <td>10.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>400.96000</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>36.86272</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>536.96000</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":"<div class=\"ansiout\">Out[11]: </div>","removedWidgets":[],"addedWidgets":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n      <th>Id</th>\n      <th>Month</th>\n      <th>Weekday</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-33429.70000</td>\n      <td>13.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35.68625</td>\n      <td>10.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>400.96000</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>36.86272</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>536.96000</td>\n      <td>3.0</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["forest_model = forest_train_save(train_logs)\nprint(forest_model)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f0d1d10-7f44-49a5-ad36-9acea8d2c94f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/context.py:77: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  DeprecationWarning)\n2020/11/28 14:03:36 WARNING mlflow.sklearn: Failed to infer model signature: could not sample data to infer model signature: &#39;y&#39;\nTrue\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/context.py:77: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  DeprecationWarning)\n2020/11/28 14:03:36 WARNING mlflow.sklearn: Failed to infer model signature: could not sample data to infer model signature: &#39;y&#39;\nTrue\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"train_save_isoforest","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1160269160420946}},"nbformat":4,"nbformat_minor":0}
